#include <pybind11/pybind11.h>
#include <pybind11/functional.h>
#include <pybind11/operators.h>
#include <pybind11/stl.h>

#include <cxbind/cxbind.h>
#include <crunge/wgpu/pywgpu.h>
#include <crunge/wgpu/crunge-wgpu.h>
#include <crunge/wgpu/conversions.h>

struct LinearAlloc {
    uint8_t* base = nullptr;
    size_t cap = 0, off = 0;

    explicit LinearAlloc(size_t capacity = 64 * 1024) {
        base = (uint8_t*)std::malloc(capacity);
        if (!base) throw std::bad_alloc();
        cap = capacity;
    }
    ~LinearAlloc() { std::free(base); }

    void* alloc(size_t n, size_t align) {
        size_t p = (off + (align - 1)) & ~(align - 1);
        if (p + n > cap) throw std::runtime_error("LinearAlloc overflow");
        off = p + n;
        return base + p;
    }

    template<class T>
    T* alloc_array(size_t count) {
        return reinterpret_cast<T*>(alloc(sizeof(T) * count, alignof(T)));
    }

    template<class T>
    T* make() {
        void* p = alloc(sizeof(T), alignof(T));
        return new (p) T{}; // value-init
    }

    template<class T>
    T* make_array(size_t count) {
        T* p = alloc_array<T>(count);
        for (size_t i = 0; i < count; ++i) {
            new (&p[i]) T{}; // value-init each element
        }
        return p;
    }

    const char* copy_cstr(const std::string& s) {
        char* p = (char*)alloc(s.size() + 1, alignof(char));
        std::memcpy(p, s.c_str(), s.size() + 1);
        return p;
    }
};

struct BuildCtx {
    LinearAlloc& la;
    //const ChainRegistry* chains = nullptr; // optional; nullptr if you don't support nextInChain yet
};

template <class T>
struct Builder {
    BuildCtx ctx;

    explicit Builder(BuildCtx c) : ctx(c) {}

    // Allocate one T and fill it.
    T* build(py::handle dc) {
        auto* p = ctx.la.make<T>();
        fill(*p, dc);
        return p;
    }

    // Allocate N Ts and fill each element.
    T* build_array(py::handle seq_h, uint32_t* outCount) {
        if (seq_h.is_none()) {
            *outCount = 0;
            return nullptr;
        }
        py::sequence seq = seq_h.cast<py::sequence>();
        *outCount = static_cast<uint32_t>(seq.size());
        if (*outCount == 0) return nullptr;

        T* arr = ctx.la.make_array<T>(*outCount);
        for (uint32_t i = 0; i < *outCount; ++i) {
            fill(arr[i], seq[i]);
        }
        return arr;
    }

    // Default fill must be specialized per T (your generator writes specializations).
    void fill(T&, py::handle) {
        static_assert(sizeof(T) == 0, "Builder<T>::fill must be specialized/generated for this T");
    }

    // Common helpers
    const char* str_or_null(py::handle h) {
        if (h.is_none()) return nullptr;
        return ctx.la.copy_cstr(std::string(py::str(h)));
    }

    template <class H>
    H handle(py::handle h) { return h.cast<H>(); }

    template <class U>
    U* optional_ptr(py::handle h) {
        if (h.is_none()) return nullptr;
        return Builder<U>(ctx).build(h);
    }
};

namespace py = pybind11;

using namespace pywgpu;

{{ prologue }}

void init_wgpu_py_auto(py::module &m, Registry &registry) {
{{ py_code }}
}
